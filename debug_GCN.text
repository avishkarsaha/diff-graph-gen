step 0	- simply projecting input adjacency from 1-->3-->1 dims doesnt work,
	  as resultant adjacency tends to 0 during training. why? (12% acc)
        - all parameters of model receiving 0 gradients --- this seems to be because
          of relu applied to edge probabilities, using sigmoid made gradients non-zero
        - obviously sigmoid should be applied to SPARSE tensor, as it makes 0 values 0.5
          this brings accuracy back to around 78% (78%)
        - step0_01 - using edge features composed of x features at its edges brings acc
          down (70% acc)
step 1	- perturbing edge probabilities with noise maintains acc (78% acc)
step 2  - predicting k and then selecting top k reduces acc a little (75% acc)
step 3	- k_only_w_linear_grad brings acc down (65% acc)
step 4	- k_times_edge_prob gets SOTA (81% acc)

2 DGG layers, with input_adj for each DGG layer
	- acc plateaus around 79%, lower than our 81% best
2 DGG layers, with previous_adj for each DGG layer
	- does slightly better than using input_Adj for each DGG layer



